import gradio as gr  # ê·¸ë¼ë””ì˜¤ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.
from transformers import AutoTokenizer, AutoModelForCausalLM

tokenizer = AutoTokenizer.from_pretrained("beomi/gemma-ko-7b", cache_dir="./gemma-ko-7b")
model = AutoModelForCausalLM.from_pretrained("gemma-ko-7b", device_map="auto", cache_dir="./gemma-ko-7b")

# ì±—ë´‡ì— ì±„íŒ…ì´ ì…ë ¥ë˜ë©´ ì´ í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•©ë‹ˆë‹¤. 
# messageëŠ” ìœ ì €ì˜ ì±„íŒ… ë©”ì‹œì§€, historyëŠ” ì±„íŒ… ê¸°ë¡, additional_input_infoëŠ” additional_inputsì•ˆ ë¸”ë¡ì˜ ì •ë³´ë¥¼ ë°›ìŠµë‹ˆë‹¤.
def response(message, history, additional_input_info):
    # additional_input_infoì˜ í…ìŠ¤íŠ¸ë¥¼ ì±—ë´‡ì˜ ëŒ€ë‹µ ë’¤ì— ì¶”ê°€í•©ë‹ˆë‹¤.
    input_ids = tokenizer(message, return_tensors="pt").to("cuda")
    outputs = model.generate(**input_ids)
    return tokenizer.decode(outputs[0])

gr.ChatInterface(
        fn=response,
        textbox=gr.Textbox(placeholder="ë§ê±¸ì–´ì£¼ì„¸ìš”..", container=False, scale=7),
        title="ì–´ë–¤ ì±—ë´‡ì„ ì›í•˜ì‹¬ë¯¸ê¹Œ?",
        description="ë¬¼ì–´ë³´ë©´ ë‹µí•˜ëŠ” ì±—ë´‡ì„ë¯¸ë‹¤.",
        theme="soft",
        examples=[["ì•ˆë‡½"], ["ìš”ì¦˜ ë¥ë‹¤ ã… ã… "], ["ì ì‹¬ë©”ë‰´ ì¶”ì²œë°”ëŒ, ì§œì¥ ì§¬ë½• íƒ 1"]],
        retry_btn="ë‹¤ì‹œë³´ë‚´ê¸° â†©",
        undo_btn="ì´ì „ì±— ì‚­ì œ âŒ",
        clear_btn="ì „ì±— ì‚­ì œ ğŸ’«",
        additional_inputs=[
            gr.Textbox("!!!", label="ëë§ì‡ê¸°")
        ]
).launch()